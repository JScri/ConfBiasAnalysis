#!/usr/bin/env python3
"""
RT by Bias Type Analysis for BCI Confirmation Bias Study
VERSION 2.0 - READS FROM BEHAVIORAL ANALYSIS CSV OUTPUTS

Analyzes reaction time differences across bias classifications:
- CONFIRMATION_BIAS
- COUNTER_BIAS  
- MIXED
- NEUTRAL
- DISCONFIRMATION_SEEKING (if any)

This version integrates with your existing pipeline by reading from
articles_with_bias.csv files generated by bci_analysis_behavioural.py

Author: Jason Stewart
Ethics: ETH23-7909
Date: November 2, 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 11

class BiasTypeRTAnalyzer:
    """Analyze reaction times by bias classification type"""
    
    def __init__(self, 
                 root_dir: str = './bci_output/patched',
                 output_dir: str = './bci_output/patched/rt_by_bias_analysis'):
        self.root_dir = Path(root_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.article_csv_files = []
        self.combined_df = None
        self.bias_type_summary = None
    
    def find_csv_files(self) -> bool:
        """Auto-discover articles_with_bias.csv files in test_session* folders"""
        print("="*70)
        print("AUTO-DISCOVERING BEHAVIORAL ANALYSIS CSV FILES")
        print("="*70)
        print(f"Searching in: {self.root_dir}")
        
        # Look for test_session* folders
        test_session_folders = sorted(self.root_dir.glob('test_session*'))
        
        if not test_session_folders:
            print(f"\n⚠ No test_session* folders found in {self.root_dir}")
            print("  Trying alternative search in current directory...")
            test_session_folders = sorted(Path('.').glob('**/test_session*'))
        
        print(f"\nFound {len(test_session_folders)} test_session folders:")
        
        for folder in test_session_folders:
            # Look for articles_with_bias.csv in this folder
            csv_file = folder / 'articles_with_bias.csv'
            
            if csv_file.exists():
                self.article_csv_files.append(csv_file)
                print(f"  ✓ {folder.name:20s} → articles_with_bias.csv")
            else:
                print(f"  ⚠ {folder.name:20s} → No CSV file found")
        
        if not self.article_csv_files:
            print("\n⚠ No articles_with_bias.csv files found!")
            print("\nSearched for: test_session*/articles_with_bias.csv")
            print(f"In directory: {self.root_dir.absolute()}")
            print("\nMake sure you've run bci_analysis_behavioural.py first!")
        else:
            print(f"\n✓ Found {len(self.article_csv_files)} CSV files total")
        
        return len(self.article_csv_files) > 0
    
    def load_and_combine_data(self) -> pd.DataFrame:
        """Load all CSV files and combine into single DataFrame"""
        print("\n" + "="*70)
        print("LOADING ARTICLE-LEVEL DATA")
        print("="*70)
        
        if not self.article_csv_files:
            print("⚠ No CSV files to load (run find_csv_files() first)")
            return None
        
        all_data = []
        
        for csv_file in self.article_csv_files:
            # Extract participant ID from folder name
            session_name = csv_file.parent.name
            participant_id = session_name.replace('test_session', '').replace('_', '')
            
            print(f"  Loading P{participant_id}...", end='')
            
            # Load CSV
            df = pd.read_csv(csv_file)
            
            # Add participant ID
            df['participant_id'] = participant_id
            
            # Filter valid articles only (passed attention checks)
            df_valid = df[df['valid'] == 1].copy()
            
            # Only include articles with bias classification
            df_valid = df_valid[df_valid['bias_type'].notna()].copy()
            
            print(f" {len(df_valid)} valid articles")
            all_data.append(df_valid)
        
        # Combine all participants
        self.combined_df = pd.concat(all_data, ignore_index=True)
        
        print(f"\n✓ Loaded {len(self.combined_df)} total valid articles from {len(self.article_csv_files)} participants")
        
        # Rename reading_time to reaction_time for consistency
        if 'reading_time' in self.combined_df.columns:
            self.combined_df['reaction_time'] = self.combined_df['reading_time']
        
        return self.combined_df
    
    def analyze_rt_by_bias_type(self) -> pd.DataFrame:
        """Analyze RT differences across bias types"""
        print("\n" + "="*70)
        print("RT BY BIAS TYPE ANALYSIS")
        print("="*70)
        
        if self.combined_df is None or len(self.combined_df) == 0:
            print("⚠ No data loaded")
            return None
        
        # Group by bias type
        self.bias_type_summary = self.combined_df.groupby('bias_type')['reaction_time'].agg([
            ('count', 'count'),
            ('mean_rt', 'mean'),
            ('std_rt', 'std'),
            ('median_rt', 'median'),
            ('min_rt', 'min'),
            ('max_rt', 'max')
        ]).round(2)
        
        print("\nBias Type Summary:")
        print("-"*70)
        print(self.bias_type_summary.to_string())
        
        # Calculate percentage of each bias type
        total_articles = len(self.combined_df)
        print(f"\n\nBias Type Distribution:")
        print("-"*70)
        for bias_type in self.bias_type_summary.index:
            count = self.bias_type_summary.loc[bias_type, 'count']
            percentage = (count / total_articles) * 100
            mean_rt = self.bias_type_summary.loc[bias_type, 'mean_rt']
            print(f"  {bias_type:25s}: {count:3d} articles ({percentage:5.1f}%) | Mean RT: {mean_rt:.1f}s")
        
        return self.combined_df
    
    def statistical_comparisons(self):
        """Perform statistical tests comparing bias types"""
        print("\n" + "="*70)
        print("STATISTICAL COMPARISONS")
        print("="*70)
        
        # Get bias types with sufficient data (n >= 3)
        bias_counts = self.combined_df['bias_type'].value_counts()
        valid_types = bias_counts[bias_counts >= 3].index.tolist()
        
        if len(valid_types) < 2:
            print("⚠ Insufficient data for statistical comparison (need 2+ bias types with n>=3)")
            return
        
        print(f"\nBias types with sufficient data (n>=3): {', '.join(valid_types)}")
        
        # Kruskal-Wallis test (non-parametric ANOVA)
        groups = [self.combined_df[self.combined_df['bias_type'] == bt]['reaction_time'].values 
                  for bt in valid_types]
        h_stat, p_value = stats.kruskal(*groups)
        
        print(f"\n1. Kruskal-Wallis Test (non-parametric ANOVA):")
        print(f"   H-statistic = {h_stat:.3f}")
        print(f"   p-value = {p_value:.4f}")
        
        if p_value < 0.05:
            print(f"   ✓ SIGNIFICANT: Bias types differ in RT (p < 0.05)")
        else:
            print(f"   - Not significant: No RT difference detected")
        
        # Pairwise comparisons (Mann-Whitney U tests)
        if len(valid_types) >= 2:
            print(f"\n2. Pairwise Comparisons (Mann-Whitney U tests):")
            print("-"*70)
            
            for i, type1 in enumerate(valid_types):
                for type2 in valid_types[i+1:]:
                    rt1 = self.combined_df[self.combined_df['bias_type'] == type1]['reaction_time'].values
                    rt2 = self.combined_df[self.combined_df['bias_type'] == type2]['reaction_time'].values
                    
                    u_stat, p_val = stats.mannwhitneyu(rt1, rt2, alternative='two-sided')
                    
                    mean1 = np.mean(rt1)
                    mean2 = np.mean(rt2)
                    diff = mean1 - mean2
                    
                    sig = "✓" if p_val < 0.05 else "-"
                    print(f"  {sig} {type1:20s} vs {type2:20s}:")
                    print(f"     Mean diff: {diff:+6.1f}s | p = {p_val:.4f}")
        
        # Effect sizes (Cohen's d)
        if len(valid_types) >= 2:
            print(f"\n3. Effect Sizes (Cohen's d):")
            print("-"*70)
            
            max_d = 0
            max_comparison = None
            
            for i, type1 in enumerate(valid_types):
                for type2 in valid_types[i+1:]:
                    rt1 = self.combined_df[self.combined_df['bias_type'] == type1]['reaction_time'].values
                    rt2 = self.combined_df[self.combined_df['bias_type'] == type2]['reaction_time'].values
                    
                    # Calculate Cohen's d
                    mean_diff = np.mean(rt1) - np.mean(rt2)
                    pooled_std = np.sqrt((np.std(rt1, ddof=1)**2 + np.std(rt2, ddof=1)**2) / 2)
                    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0
                    
                    print(f"  {type1:20s} vs {type2:20s}: d = {cohens_d:+.3f}")
                    
                    if abs(cohens_d) > abs(max_d):
                        max_d = cohens_d
                        max_comparison = (type1, type2)
            
            if max_comparison:
                print(f"\n  Largest effect: {max_comparison[0]} vs {max_comparison[1]} (d = {max_d:+.3f})")
                if abs(max_d) > 0.8:
                    print(f"  → Large effect size")
                elif abs(max_d) > 0.5:
                    print(f"  → Medium effect size")
                elif abs(max_d) > 0.2:
                    print(f"  → Small effect size")
                else:
                    print(f"  → Negligible effect size")
    
    def create_visualizations(self):
        """Create RT by bias type visualizations"""
        print("\n" + "="*70)
        print("CREATING VISUALIZATIONS")
        print("="*70)
        
        bias_types = sorted(self.combined_df['bias_type'].unique())
        
        # Define colors for bias types
        colors = {
            'CONFIRMATION_BIAS': '#e74c3c',
            'COUNTER_BIAS': '#3498db',
            'MIXED': '#95a5a6',
            'NEUTRAL': '#f39c12',
            'DISCONFIRMATION_SEEKING': '#2ecc71'
        }
        
        # 1. Box plot with individual points
        fig1, ax1 = plt.subplots(figsize=(12, 7))
        
        positions = range(len(bias_types))
        bp = ax1.boxplot([self.combined_df[self.combined_df['bias_type'] == bt]['reaction_time'].values 
                           for bt in bias_types],
                         positions=positions,
                         labels=bias_types,
                         patch_artist=True,
                         widths=0.6)
        
        # Color boxes
        for patch, bias_type in zip(bp['boxes'], bias_types):
            patch.set_facecolor(colors.get(bias_type, '#95a5a6'))
            patch.set_alpha(0.7)
        
        # Overlay individual points with jitter
        for i, bias_type in enumerate(bias_types):
            rt_values = self.combined_df[self.combined_df['bias_type'] == bias_type]['reaction_time'].values
            x = np.random.normal(i, 0.04, size=len(rt_values))
            ax1.scatter(x, rt_values, alpha=0.5, s=50, color='black', zorder=3)
        
        ax1.set_ylabel('Reaction Time (seconds)', fontsize=13, fontweight='bold')
        ax1.set_xlabel('Bias Type', fontsize=13, fontweight='bold')
        ax1.set_title('Article Reading Time by Bias Classification', fontsize=15, fontweight='bold')
        ax1.grid(axis='y', alpha=0.3)
        
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(self.output_dir / '1_rt_by_bias_boxplot.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ✓ Box plot saved")
        
        # 2. Bar plot with error bars
        fig2, ax2 = plt.subplots(figsize=(12, 7))
        
        means = [self.combined_df[self.combined_df['bias_type'] == bt]['reaction_time'].mean() 
                 for bt in bias_types]
        stds = [self.combined_df[self.combined_df['bias_type'] == bt]['reaction_time'].std() 
                for bt in bias_types]
        counts = [len(self.combined_df[self.combined_df['bias_type'] == bt]) 
                  for bt in bias_types]
        
        bars = ax2.bar(positions, means, yerr=stds, capsize=5,
                      color=[colors.get(bt, '#95a5a6') for bt in bias_types],
                      alpha=0.7, edgecolor='black', linewidth=1.5)
        
        # Add count labels on bars
        for bar, count, mean in zip(bars, counts, means):
            ax2.text(bar.get_x() + bar.get_width()/2, mean + max(stds)*0.05,
                    f'n={count}', ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        # Add mean value labels
        for bar, mean in zip(bars, means):
            ax2.text(bar.get_x() + bar.get_width()/2, mean/2,
                    f'{mean:.1f}s', ha='center', va='center', 
                    fontsize=11, fontweight='bold', color='white')
        
        ax2.set_ylabel('Mean Reaction Time (seconds)', fontsize=13, fontweight='bold')
        ax2.set_xlabel('Bias Type', fontsize=13, fontweight='bold')
        ax2.set_title('Mean Article RT by Bias Type (±SD)', fontsize=15, fontweight='bold')
        ax2.set_xticks(positions)
        ax2.set_xticklabels(bias_types, rotation=45, ha='right')
        ax2.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / '2_rt_by_bias_barplot.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ✓ Bar plot saved")
        
        # 3. Violin plot
        fig3, ax3 = plt.subplots(figsize=(12, 7))
        
        parts = ax3.violinplot([self.combined_df[self.combined_df['bias_type'] == bt]['reaction_time'].values 
                                for bt in bias_types],
                               positions=positions,
                               widths=0.7,
                               showmeans=True,
                               showmedians=True)
        
        # Color violin plots
        for i, (pc, bias_type) in enumerate(zip(parts['bodies'], bias_types)):
            pc.set_facecolor(colors.get(bias_type, '#95a5a6'))
            pc.set_alpha(0.7)
        
        ax3.set_ylabel('Reaction Time (seconds)', fontsize=13, fontweight='bold')
        ax3.set_xlabel('Bias Type', fontsize=13, fontweight='bold')
        ax3.set_title('RT Distribution by Bias Type (Violin Plot)', fontsize=15, fontweight='bold')
        ax3.set_xticks(positions)
        ax3.set_xticklabels(bias_types, rotation=45, ha='right')
        ax3.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / '3_rt_by_bias_violin.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  ✓ Violin plot saved")
        
        # 4. Per-participant breakdown
        participants = sorted(self.combined_df['participant_id'].unique())
        
        if len(participants) > 1:
            fig4, ax4 = plt.subplots(figsize=(14, 8))
            
            x = np.arange(len(bias_types))
            width = 0.8 / len(participants)  # Dynamic width based on number of participants
            
            for i, pid in enumerate(participants):
                p_data = self.combined_df[self.combined_df['participant_id'] == pid]
                p_means = [p_data[p_data['bias_type'] == bt]['reaction_time'].mean() 
                          if len(p_data[p_data['bias_type'] == bt]) > 0 else 0
                          for bt in bias_types]
                
                offset = (i - len(participants)/2) * width
                ax4.bar(x + offset, p_means, width, label=f'P{pid}', alpha=0.8)
            
            ax4.set_ylabel('Mean RT (seconds)', fontsize=13, fontweight='bold')
            ax4.set_xlabel('Bias Type', fontsize=13, fontweight='bold')
            ax4.set_title('RT by Bias Type: Per-Participant Breakdown', fontsize=15, fontweight='bold')
            ax4.set_xticks(x)
            ax4.set_xticklabels(bias_types, rotation=45, ha='right')
            ax4.legend(title='Participant', bbox_to_anchor=(1.05, 1), loc='upper left')
            ax4.grid(axis='y', alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.output_dir / '4_rt_by_bias_per_participant.png', dpi=300, bbox_inches='tight')
            plt.close()
            print("  ✓ Per-participant plot saved")
        
        print(f"\n✓ All figures saved to: {self.output_dir}")
    
    def export_detailed_data(self):
        """Export detailed data for further analysis"""
        print("\n" + "="*70)
        print("EXPORTING DATA")
        print("="*70)
        
        # Save combined article-level data
        self.combined_df.to_csv(self.output_dir / 'article_level_data.csv', index=False)
        print(f"  ✓ Raw data: article_level_data.csv")
        
        # Save summary statistics
        self.bias_type_summary.to_csv(self.output_dir / 'bias_type_rt_summary.csv')
        print(f"  ✓ Summary stats: bias_type_rt_summary.csv")
        
        # Save per-participant summary
        participant_summary = self.combined_df.groupby(['participant_id', 'bias_type'])['reaction_time'].agg([
            'count', 'mean', 'std', 'median'
        ]).round(2)
        participant_summary.to_csv(self.output_dir / 'per_participant_bias_rt.csv')
        print(f"  ✓ Per-participant: per_participant_bias_rt.csv")
    
    def run_complete_analysis(self):
        """Run the complete RT by bias type analysis"""
        print("\n" + "="*70)
        print("BCI RT BY BIAS TYPE ANALYSIS")
        print("="*70)
        print(f"Output directory: {self.output_dir}\n")
        
        # Discover CSV files
        found = self.find_csv_files()
        if not found:
            print("\n⚠ No CSV files found. Check directory structure.")
            print("\nExpected structure:")
            print("  root_dir/")
            print("    test_session_11/")
            print("      articles_with_bias.csv  ← Generated by bci_analysis_behavioural.py")
            print("      participant_11.json")
            print("      test_session_11.xdf")
            print("    test_session_12/")
            print("      articles_with_bias.csv")
            print("      participant_12.json")
            print("      test_session_12.xdf")
            print("    ...")
            return
        
        # Load and combine data
        df = self.load_and_combine_data()
        
        if df is None or len(df) == 0:
            print("\n⚠ No valid data found in CSV files.")
            return
        
        # Analyze
        self.analyze_rt_by_bias_type()
        
        # Statistical tests
        self.statistical_comparisons()
        
        # Visualizations
        self.create_visualizations()
        
        # Export data
        self.export_detailed_data()
        
        print("\n" + "="*70)
        print("ANALYSIS COMPLETE")
        print("="*70)
        print(f"\nAll outputs saved to: {self.output_dir}")
        print("\nGenerated files:")
        print("  • 1_rt_by_bias_boxplot.png")
        print("  • 2_rt_by_bias_barplot.png")
        print("  • 3_rt_by_bias_violin.png")
        print("  • 4_rt_by_bias_per_participant.png")
        print("  • article_level_data.csv")
        print("  • bias_type_rt_summary.csv")
        print("  • per_participant_bias_rt.csv")


if __name__ == "__main__":
    import sys
    
    # Parse command line arguments
    root_dir = sys.argv[1] if len(sys.argv) > 1 else './bci_output/patched/'
    
    print("\n" + "="*70)
    print("RT BY BIAS TYPE ANALYSIS - STARTING")
    print("="*70)
    print(f"\nRoot directory: {root_dir}")
    print("Searching for: test_session*/articles_with_bias.csv")
    
    analyzer = BiasTypeRTAnalyzer(root_dir=root_dir)
    analyzer.run_complete_analysis()